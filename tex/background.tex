\section{Background}
\label{background}

The stochastic multi-armed bandit problem is an important model for studying the exploration-exploitation trade-off in reinforcement learning. The goal of the decision-maker here is to obtain the maximum number of rewards over a number of trials. This report presents an empirical study of the popular 2-armed bandit problem. Our work is motivated from [\ref{original}]. The paper presents a variety of existing heuristic methods for a finite horizon 2-arm bandit problem. It then builds on the idea of latent full state modeling and presents a new, simplified model that captures the results of the latent full state model. It uses a Bayesian model in which how people balance exploration with exploitation depends on his or her assumptions about the distribution of reward rates. 



 