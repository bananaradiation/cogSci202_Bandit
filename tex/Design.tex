\section{Goals}
The goals of our experimental setup are 3-fold: generate optimal data for the bandit problem given different environment settings, fit parameter values of different heuristic models to optimal model to get best heuristic decision sequences, and the comparison of these with the optimal and human data.

\section{Implementation}
\subsection{Experiment Setup}
Our dataset \cite{data} consists of 50 experiments on 3 environments: ‘plentiful’($\alpha=4,\beta=2$) , ‘neutral’($\alpha=1,\beta=1$) and ‘scarce’($\alpha=2,\beta=4$). $\alpha$ and $\beta$ define the count of prior successes and failures, respectively. Each experiment consists of 8 trials and the reward rates $\mu_k$ for arm $k$ is drawn from the beta distribution, i.e. $\mu_k \sim Beta (\alpha, \beta)$. Thus, we draw a total of 100 reward rates in this setting (50 for each arm). For the $i^{th}$ trial in the $t^{th}$ experiment, if arm $k$ is chosen, the reward $R_i^t$ is determined by the outcome of a Bernoulli using the reward rate $\mu_k^t$, so $R_i^t \sim Bernoulli (\mu_k^t)$.

\subsection{Optimal Model}
For a small number of trials and a fixed environment, the optimal solution can be obtained by following a dynamic programming approach as given in \cite{kaelbling},\cite{steyvers}. The state of the $t$th trial can be defined as $S_t = <s_1,s_2,f_1,f_2>$, where $s_1$ \& $s_2$ are the number of successes for arm 1 \& 2, respectively, and $f_1$ \& $f_2$ are the number of failures for arm 1 \& 2, respectively, up to trial $t$. For a given environment $\alpha,\beta$, we can recursively define the value function as the expected future reward for state $S_t$ plus the current reward at state $S_t$ as follows : 
\begin{align*}
	V_t(s_1,s_2,f_1,f_2) &= \max_{i \in \{1,2\}} 
	\Big(
	\frac{s_i + \alpha}{s_i + f_i + \alpha + \beta} V_{t+1}(...,s_i+1,...) \\
	&+ \frac{f_i + \beta}{s_i + f_i + \alpha + \beta} V_{t+1}(...,f_i+1,...)\\
	&+ \frac{s_i + \alpha}{s_i + f_i + \alpha + \beta} 
	\Big)	
\end{align*}


The state value function for the last trial $V_T=\max_{i \in \{1,2\}} (\frac{s_i + \alpha}{s_i + f_i + \alpha + \beta})$ completes the recursive definition. Using this definition, the value function is computed for all enumerable states from $t=7\ldots1$. To obtain the optimal decision, the argument that maximizes the state value function is selected, thus giving the best arm to be chosen for each possible state. 

To obtain a set of decisions for each trial, we sample the reward rate $\mu_i$ for each arm $i$ from the known Beta distribution of that environment. The optimal decision $i$ for each trial combined with the reward sampled from the Bernoulli distribution given by $\mu_i$ gives us the next state.

\subsection{Heuristic Models}
We implement the five heuristics used in \cite{shunan2011} and summarize the key parameters and their meanings in the following table: 

\begin{table}[h]
\begin{tabular}{|l|c|l|}
\hline
\textbf{Heuristic} & \textbf{Key Parameter} & \textbf{Meaning} \\ \hline
$\epsilon$-greedy & $\epsilon$ & probability of exploration \\ \hline
$\epsilon$-decreasing & $\epsilon_0$ & probability of exploration in 1st trial \\ \hline
WSLS & $\gamma$ & probability of staying after winning and shifting after losing \\ \hline
Full latent state & $\theta$ & multifaceted, see \cite{shunan2009} \\ \hline
$\tau$-switch & $\tau$ & trial \# for switching from exploration to exploitation \\ \hline
\end{tabular}
\end{table}

\subsection{Heuristic Model Comparisons}
We use grid search to find the key parameter or the environment parameters for each heuristic. To do this, we define an $L$-value similar to a likelihood function of a Bernoulli function. This comparison differs from the original paper which infers parameters by calculating a posterior predictive average agreement via sampling. For the optimal data, we perform grid search over the key parameter and the heuristic decision sequences that yield the highest $L$-value; while for the human data, we assume that the key parameter value is identified from the optimal data, and grid search over the $\alpha',\beta'$ parameters and the heuristic decision sequences that yield the highest $L$-value. Without loss of generality, the following approach is explained based on the $\epsilon$-greedy heuristic. 

We derive our method for model fitting to optimal and human data based on three key factors: (1) there are a finite number of possible decision sequences; (2) we constraint the possible combinations of environment variables, $\alpha'$ and $\beta'$; and (3) an assumption of conditional independence of the underlying arm's reward distribution and the action sequence. Each decision maker has a continuously updating belief of the environment variabls, $\alpha',\beta'$. In the case of the optimal decision maker, $\alpha',\beta'$ match the true environment.

We define the $L$-value as :
\begin{align}
\begin{split}
	\label{eq:likelihood} 
	L(Action,Reward|Data) &= P(Action,Reward|Data) \\
						  &= \prod_{t} P(f(\epsilon),f(\mu_k)|Data) \\
						  &= \prod_{t} P(f(\epsilon)|Data) P(f(\mu_k)|Data) \\	
						  &= \prod_{t} \epsilon^x (1-\epsilon)^{1-x} \mu_k^y (1-\mu_k)^{1-y}
\end{split}
\end{align}
where data corresponds to the observed decision and reward sequence of a decison maker encoded in the binary value for $x$ and $y$, respectively. The random variables for action and reward are functions depending on the heuristic parameter, $\epsilon$, and arm $k$'s reward probability, $\mu_k$. Additionally the actions are based on rules defined by each heuristic. Here we make the assumption that the probabiliy of the random variables Action and Reward given data are independent. This assumption stems from our intuition that given data for a decision sequence, the probabilities for $\epsilon$ and $\mu$ are independent. 

\subsubsection{Fitting Heuristic Decision to Optimal Data}
We assume the optimal decision-maker has perfect knowledge of the environment, i.e. the $\alpha$ and $\beta$ values are known, and thus the arm's reward probability. This makes sense as we want to characterize a heuristic's decision making process and see how well it compares to the best possible decision. To find the best fit for each of the heuristic methods, we perform grid search for its key parameter value from all enumerated sequences. For example, the best $\epsilon$ parameter is found by averaging the results of the grid search which resulted in the maximum $L$-value. Finally that particular heuristic decision sequence is compared to the optimal decision sequence using match percentage for assessment. A comparison of these decisions with the optimal decisions gives us an idea of each heuristic's performance.

\subsubsection{Fitting Heuristic Decision to Human Data}
The assumption of conditional independence of $\epsilon$ and $\mu$ may not hold true given human data, but if a heuristic is indeed capturing the best decision sequence, a human's expectation of $\mu$ would match the arm's true reward probability.

Using this, we grid search over three factors that yield the maximum $L$-value: a sequence of decisions, and the human's belief of the two environment variables. There are a finite number of decision sequences, specifically $2^8$ for an 8-trial experiment, and we constrain our environment with $\alpha'+\beta'=10$ to simplify the search process. Finally, we fit the human data to the optimal heuristic parameter and sequence to determine a measure of agreement averaged over all games.

\subsection{Potential Design Issues}
Originally we did grid search based on decision match percentage compared to the optimal model decision. However, averaging our parameter value for multiple iterations yielded results with wide variation. One reason match percentage gave inconclusive results is because many different parameter values could result in 100\% match. If the best match results in just one decision being off (e.g. a match percentage of 87.5\%), there could eight different decision sequences that differ from the optimal sequence by one. This mean there could be \textit{at least} eight different $\epsilon$ parameters that result in that match percentage. We could improve our method by tracking all the different decision sequences that could result from a particular match percentage and the corresponding $\epsilon$. Then each identified parameter choice would have to be simulated under the heuristic and compared. Regardless of the resulting increase in complexity, we would still need to define a method to compare all these parameter choices, and it is not clear how to define a way to differentiate the parameters for identifying an optimal value.

Thus, we formulated the idea for $L$-value. While we ultimately report tentative results using the method of $L$-value for comparing grid search parameters, we recognize it is not fully valid for model fitting to optimal data because we cannot make the assumption of conditional independence of $\epsilon$ and $\mu$. For human data, $\mu'$ is an expectation of the arm's reward probability and that belief is continuously updated over the course of the trial; nowhere is it used to determine the reward because the data is already observed.

However, the optimal data consists of all enumerable states and the optimal decision sequence to take for each state. The reward data is drawn from the arm's reward probability so for an optimal model that data is probabilistic depending on the true $\mu$. In the optimal model, $\mu'$ is also a continuously updated expectation of the true reward probability. Even though every state can be enumerated, it is not clear how to choose which state to transition to without drawing the outcome of an action from the true $\mu$, so there are many $L$-values that could be maximal depending on the state.